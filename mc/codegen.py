from mc.graph import Graph
from mc.node import Node, IndexNode
from typing import Dict
import mc.operators as ops
import numpy as np
from mc.utils import cpp_type, CodeWriter
import os

class CodeGen(CodeWriter):
    name_dict: Dict[str, str]
    codegen_dir: str
    data_dir: str
    node_constant_tensors: Dict[str, np.ndarray]
    def __init__(self, graph: Graph, codegen_dir, data_dir):
        super().__init__()
        self.graph = graph
        self.name_dict = {}
        self.codegen_dir = codegen_dir
        self.data_dir = data_dir
        self.node_constant_tensors = {}

    def node_name(self, node: Node):
        if node.name not in self.name_dict:
            node_name = node.name
            node_name = node_name.replace('/', '_')
            node_name = node_name.replace('::', '_')
            node_name = node_name.replace('.', '_')
            if not node_name.replace('_', '').isalnum() or '__' in node_name:
                raise ValueError(f'Node name {node_name} is not valid')
            self.name_dict[node.name] = node_name
        return self.name_dict[node.name]

    def tensor_name(self, inode: IndexNode):
        return f'{self.node_name(inode.node)}__{inode.index}'
    
    def gen_constants(self):
        os.makedirs(os.path.join(self.codegen_dir, "constants"), exist_ok=True)
        for node in self.graph.nodes.values():
            if isinstance(node, ops.Constant):
                node.value.tofile(os.path.join(self.codegen_dir, "constants", f'{self.node_name(node)}.bin'))
                with open(os.path.join(self.codegen_dir, "constants", f'{self.node_name(node)}.shape'), "w") as f:
                    f.write(" ".join([str(x) for x in node.value.shape]))
        for node in self.graph.nodes.values():
            self.node_constant_tensors.update(node.get_constant_tensors(self.node_name(node)))
        for tensor_name, tensor in self.node_constant_tensors.items():
            tensor.tofile(os.path.join(self.codegen_dir, "constants", f'{tensor_name}.bin'))
            with open(os.path.join(self.codegen_dir, "constants", f'{tensor_name}.shape'), "w") as f:
                f.write(" ".join([str(x) for x in tensor.shape]))

    def write_helper_code(self):
        code_template = r'''// generated by MCCompiler
#include <cstdio>
#include <cstdlib>
#include <string>
#include <fstream>
#include <cuda.h>
#include <cublas_v2.h>
#include <PATH_TO_MC_OPERATOR_HEADER>

#define UNREACHABLE() { \
    printf("file %s line %i: unreachable!\n", __FILE__, __LINE__); \
    fflush(stdout); \
    exit(1); \
}

static const char *cublasGetErrorString(cublasStatus_t error) {
    switch (error)
    {
        case CUBLAS_STATUS_SUCCESS:
            return "CUBLAS_STATUS_SUCCESS";
        case CUBLAS_STATUS_NOT_INITIALIZED:
            return "CUBLAS_STATUS_NOT_INITIALIZED";
        case CUBLAS_STATUS_ALLOC_FAILED:
            return "CUBLAS_STATUS_ALLOC_FAILED";
        case CUBLAS_STATUS_INVALID_VALUE:
            return "CUBLAS_STATUS_INVALID_VALUE";
        case CUBLAS_STATUS_ARCH_MISMATCH:
            return "CUBLAS_STATUS_ARCH_MISMATCH";
        case CUBLAS_STATUS_MAPPING_ERROR:
            return "CUBLAS_STATUS_MAPPING_ERROR";
        case CUBLAS_STATUS_EXECUTION_FAILED:
            return "CUBLAS_STATUS_EXECUTION_FAILED";
        case CUBLAS_STATUS_INTERNAL_ERROR:
            return "CUBLAS_STATUS_INTERNAL_ERROR";
        case CUBLAS_STATUS_NOT_SUPPORTED:
            return "CUBLAS_STATUS_NOT_SUPPORTED";
        case CUBLAS_STATUS_LICENSE_ERROR:
            return "CUBLAS_STATUS_LICENSE_ERROR";
        default:
            return "<unknown>";
    }
    UNREACHABLE()
}

#define checkCudaErrors(stmt) {                                 \
    cudaError_t err = stmt;                            \
    if (err != cudaSuccess) {                          \
    fprintf(stderr, "%s in file %s, function %s, line %i: %04d %s\n", #stmt, __FILE__, __FUNCTION__, __LINE__, err, cudaGetErrorString(err)); \
    exit(1); \
    }                                                  \
}

#define checkBlasErrors(stmt) { \
    cublasStatus_t err = stmt; \
    if (err != CUBLAS_STATUS_SUCCESS) {                          \
    fprintf(stderr, "%s in file %s, function %s, line %i: %04d %s\n", #stmt, __FILE__, __FUNCTION__, __LINE__, err, cublasGetErrorString(err)); \
    exit(1); \
    } \
}

void load_tensor(std::string f_name, void* buffer, size_t size, bool on_gpu=true) {
    std::ifstream f(f_name.c_str(), std::ios::in | std::ios::binary);
    if (!f.is_open()) {
        fprintf(stderr, "Cannot open file %s\n", f_name.c_str());
        exit(1);
    }
    if (on_gpu) {
        void* tmp_buffer = malloc(size);
        f.read((char*)tmp_buffer, size);
        checkCudaErrors(cudaMemcpy(buffer, tmp_buffer, size, cudaMemcpyHostToDevice));
        free(tmp_buffer);
    } else {
        f.read((char*) buffer, size);
    }
    f.close();
}

void check_equal_cpu(std::string f_name, float* out, float* ref, size_t size) {
    float eps = 1e-5;
    for (size_t i = 0; i < size; i++) {
        if (out[i] - ref[i] > eps || ref[i] - out[i] > eps) {
            fprintf(stderr, "Error: %s at %d: %f != %f\n", f_name.c_str(), i, out[i], ref[i]);
            exit(1);
        }
    }
}

void print_tensor_cpu(std::string name, float* data, size_t size) {
    printf("%s: ", name.c_str());
    for (size_t i = 0; i < min(size, (decltype(size)) 10); i++) printf("%f ", data[i]);
    if (size > 10) { printf("..."); printf("%f ", data[size - 1]); }
    printf("\n");
}
    
'''
        code_template = code_template.replace("PATH_TO_MC_OPERATOR_HEADER", os.path.normpath(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'operators/cuda/kernel.h')))
        self.write(code_template)

    def write_init_code(self):
        node_list = self.graph.topological_sort()
        self.wl('char* cuda_buffer;')
        for node in node_list:
            if not isinstance(node, ops.Input):
                for i in range(len(node.output_types)):
                    self.wl(f'{cpp_type(node.output_types[i].dtype)}* {self.tensor_name(IndexNode(node, i))};')
        for tensor_name, tensor in self.node_constant_tensors.items():
            self.wl(f'{cpp_type(tensor.dtype)}* {tensor_name};')
        self.wl('')
        buffer_size = 0
        for node in node_list:
            if not isinstance(node, ops.Input):
                for i in range(len(node.output_types)):
                    if len(node.output_nodes[i]) == 1 and isinstance(node.output_nodes[i][0].node, ops.Output):
                        continue
                    buffer_size += node.output_types[i].size() * node.output_types[i].dtype.itemsize
        for tensor in self.node_constant_tensors.values():
            buffer_size += tensor.size * tensor.dtype.itemsize
        buffer_ptr = 0
        self.wl('void init()')
        self.block_start()
        self.wl(f'checkCudaErrors(cudaMalloc(&cuda_buffer, {buffer_size}));')
        for node in node_list:
            if not isinstance(node, ops.Input):
                for i in range(len(node.output_types)):
                    if len(node.output_nodes[i]) == 1 and isinstance(node.output_nodes[i][0].node, ops.Output):
                        continue
                    self.wl(f'{self.tensor_name(IndexNode(node, i))} = ({cpp_type(node.output_types[i].dtype)}*)(cuda_buffer + {buffer_ptr}); /* {node.output_types[i].shape} * {node.output_types[i].dtype.itemsize} */')
                    buffer_ptr += node.output_types[i].size() * node.output_types[i].dtype.itemsize
        for tensor_name, tensor in self.node_constant_tensors.items():
            self.wl(f'{tensor_name} = ({cpp_type(tensor.dtype)}*)(cuda_buffer + {buffer_ptr}); /* {tensor.shape} * {tensor.dtype.itemsize} */')
            buffer_ptr += tensor.size * tensor.dtype.itemsize
        for node in node_list:
            if isinstance(node, ops.Constant):
                self.wl(f'load_tensor("{os.path.join(self.codegen_dir, "constants", self.node_name(node))}.bin", {self.tensor_name(IndexNode(node, 0))}, {node.output_types[0].size() * node.output_types[0].dtype.itemsize});')
        for tensor_name, tensor in self.node_constant_tensors.items():
            self.wl(f'load_tensor("{os.path.join(self.codegen_dir, "constants", tensor_name)}.bin", {tensor_name}, {tensor.size * tensor.dtype.itemsize});')
        assert buffer_ptr == buffer_size
        self.block_end()
        self.wl('')


    def write_kernels(self):
        node_list = self.graph.topological_sort()
        for node in node_list:
            if isinstance(node, (ops.Input, ops.Constant, ops.Output)):
                continue
            params = [f'{cpp_type(t.dtype)}* input{i}' for i, t in enumerate(node.input_types)]
            params += [f'{cpp_type(t.dtype)}* output{i}' for i, t in enumerate(node.output_types)]
            func_sig =  f"void kernel_{self.node_name(node)}({', '.join(params)})"
            self.wl(f'// {node}')
            self.wl(f'// {node.input_types}')
            self.wl(f'// {node.output_types}')
            self.write(node.get_cuda_code(func_sig))
            self.wl('')
        
        run_params = []
        for node in self.graph.inputs:
            assert isinstance(node, ops.Input)
            run_params.append(f'{cpp_type(node.output_types[0].dtype)}* {self.tensor_name(IndexNode(node, 0))}')
        for i, node in enumerate(self.graph.outputs):
            assert isinstance(node, ops.Output)
            run_params.append(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}')
        
        self.wl(f'void run({", ".join(run_params)})')
        self.block_start()
        for node in node_list:
            if isinstance(node, (ops.Input, ops.Constant, ops.Output)):
                continue
            params = [self.tensor_name(input_node) for input_node in node.input_nodes]
            params += [self.tensor_name(IndexNode(node, i)) for i in range(len(node.output_types))]
            self.wl(f'kernel_{self.node_name(node)}({", ".join(params)});')

        self.block_end()
        self.wl('')


    def write_test_code(self):
        self.wl('int main()')
        self.block_start()
        self.wl('init();')
        for i, node in enumerate(self.graph.inputs):
            assert isinstance(node, ops.Input)
            size = node.output_types[0].size() * node.output_types[0].dtype.itemsize
            self.wl(f'{cpp_type(node.output_types[0].dtype)}* {self.tensor_name(IndexNode(node, 0))}; // input{i}')
            self.wl(f'checkCudaErrors(cudaMalloc(&{self.tensor_name(IndexNode(node, 0))}, {size}));')
            self.wl(f'load_tensor("{os.path.join(self.data_dir, f"input{i}")}.bin", {self.tensor_name(IndexNode(node, 0))}, {size});')
        for i, node in enumerate(self.graph.outputs):
            assert isinstance(node, ops.Output)
            size = node.input_types[0].size() * node.input_types[0].dtype.itemsize
            self.wl(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}; // output{i}')
            self.wl(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}_ref;')
            self.wl(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}_cpu;')
            self.wl(f'checkCudaErrors(cudaMalloc(&{self.tensor_name(node.input_nodes[0])}, {size}));')
            self.wl(f'{self.tensor_name(node.input_nodes[0])}_ref = ({cpp_type(node.input_types[0].dtype)}*) malloc({size});')
            self.wl(f'{self.tensor_name(node.input_nodes[0])}_cpu = ({cpp_type(node.input_types[0].dtype)}*) malloc({size});')
            self.wl(f'load_tensor("{os.path.join(self.data_dir, f"output{i}")}.bin", {self.tensor_name(node.input_nodes[0])}_ref, {size}, false);')
        self.wl('run(' + ', '.join([self.tensor_name(IndexNode(node, 0)) for node in self.graph.inputs] + [self.tensor_name(node.input_nodes[0]) for node in self.graph.outputs]) + ');')
        for i, node in enumerate(self.graph.outputs):
            size = node.input_types[0].size() * node.input_types[0].dtype.itemsize
            self.wl(f'checkCudaErrors(cudaMemcpy({self.tensor_name(node.input_nodes[0])}_cpu, {self.tensor_name(node.input_nodes[0])}, {size}, cudaMemcpyDeviceToHost));')
        self.wl(f'checkCudaErrors(cudaDeviceSynchronize());')
        for i, node in enumerate(self.graph.outputs):
            self.wl(f'check_equal_cpu("{self.tensor_name(node.input_nodes[0])}", {self.tensor_name(node.input_nodes[0])}_cpu, {self.tensor_name(node.input_nodes[0])}_ref, {node.input_types[0].size()});')
            self.wl(f'print_tensor_cpu("{self.tensor_name(node.input_nodes[0])}_out", {self.tensor_name(node.input_nodes[0])}_cpu, {node.input_types[0].size()});')
            self.wl(f'print_tensor_cpu("{self.tensor_name(node.input_nodes[0])}_ref", {self.tensor_name(node.input_nodes[0])}_ref, {node.input_types[0].size()});')
        self.wl('return 0;')
        self.block_end()

    def write_code(self):
        self.gen_constants()
        self.write_helper_code()
        self.write_init_code()
        self.write_kernels()
        self.write_test_code()


def codegen(graph, codegen_dir, data_dir):
    writer = CodeGen(graph, codegen_dir, data_dir)
    writer.write_code()
    print(writer.code_str)
    os.makedirs(codegen_dir, exist_ok=True)
    with open(os.path.join(codegen_dir, 'gen.cu'), 'w') as f:
        f.write(writer.code_str)
    ret = os.system(f"nvcc -std=c++11 -arch=sm_70 -O3 -o {os.path.join(codegen_dir, 'gen')} {os.path.join(codegen_dir, 'gen.cu')}")
    assert ret == 0, f"nvcc failed with code {ret}"

    # TODO: save constants
