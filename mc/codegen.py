from mc.graph import Graph
from mc.node import Node, IndexNode
from typing import Dict
import mc.operators as ops
import numpy as np
from mc.utils import cpp_type, CodeWriter
import os

def align(size: int, alignment: int):
    return ((size + alignment - 1) // alignment) * alignment

class CodeGen(CodeWriter):
    name_dict: Dict[str, str]
    codegen_dir: str
    data_dir: str
    node_constant_tensors: Dict[str, np.ndarray]
    def __init__(self, graph: Graph, codegen_dir, data_dir):
        super().__init__()
        self.graph = graph
        self.name_dict = {}
        self.codegen_dir = codegen_dir
        self.data_dir = data_dir
        self.node_constant_tensors = {}

    def node_name(self, node: Node):
        if node.name not in self.name_dict:
            node_name = node.name
            node_name = node_name.replace('/', '_')
            node_name = node_name.replace('::', '_')
            node_name = node_name.replace('.', '_')
            if not node_name.replace('_', '').isalnum() or '__' in node_name:
                raise ValueError(f'Node name {node_name} is not valid')
            self.name_dict[node.name] = node_name
        return self.name_dict[node.name]

    def tensor_name(self, inode: IndexNode):
        return f'{self.node_name(inode.node)}__{inode.index}'
    
    def gen_constants(self):
        os.makedirs(os.path.join(self.codegen_dir, "constants"), exist_ok=True)
        for node in self.graph.nodes.values():
            if isinstance(node, ops.Constant):
                node.value.tofile(os.path.join(self.codegen_dir, "constants", f'{self.node_name(node)}.bin'))
                with open(os.path.join(self.codegen_dir, "constants", f'{self.node_name(node)}.shape'), "w") as f:
                    f.write(" ".join([str(x) for x in node.value.shape]))
        for node in self.graph.nodes.values():
            self.node_constant_tensors.update(node.get_constant_tensors(self.node_name(node)))
        for tensor_name, tensor in self.node_constant_tensors.items():
            tensor.tofile(os.path.join(self.codegen_dir, "constants", f'{tensor_name}.bin'))
            with open(os.path.join(self.codegen_dir, "constants", f'{tensor_name}.shape'), "w") as f:
                f.write(" ".join([str(x) for x in tensor.shape]))

    def write_helper_code(self):
        code_template = r'''// generated by MCCompiler
#include <cstdio>
#include <cstdlib>
#include <string>
#include <fstream>
#include <cuda.h>
#include <cublas_v2.h>
#include <PATH_TO_MC_OPERATOR_HEADER>

void load_tensor(std::string f_name, void* buffer, size_t size, bool on_gpu=true) {
    std::ifstream f(f_name.c_str(), std::ios::in | std::ios::binary);
    if (!f.is_open()) {
        fprintf(stderr, "Cannot open file %s\n", f_name.c_str());
        exit(1);
    }
    if (on_gpu) {
        void* tmp_buffer = malloc(size);
        f.read((char*)tmp_buffer, size);
        checkCudaErrors(cudaMemcpy(buffer, tmp_buffer, size, cudaMemcpyHostToDevice));
        free(tmp_buffer);
    } else {
        f.read((char*) buffer, size);
    }
    f.close();
}

void check_equal_cpu(std::string f_name, float* out, float* ref, size_t size) {
    float eps = 1e-5;
    for (size_t i = 0; i < size; i++) {
        float err = fabs(out[i] - ref[i]);
        float larger = std::max(fabs(out[i]), fabs(ref[i]));
        if (err > eps && err > eps * larger) {
            fprintf(stderr, "Error: %s at %d: %f != %f\n", f_name.c_str(), i, out[i], ref[i]);
            break;
        }
    }
}

void print_tensor_cpu(std::string name, float* data, size_t size) {
    printf("%s: ", name.c_str());
    for (size_t i = 0; i < min(size, (decltype(size)) 10); i++) printf("%f ", data[i]);
    if (size > 10) { printf("..."); printf("%f ", data[size - 1]); }
    printf("\n");
}
    
'''
        code_template = code_template.replace("PATH_TO_MC_OPERATOR_HEADER", os.path.normpath(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'operators/cuda/kernel.h')))
        self.write(code_template)

    def write_init_code(self):
        node_list = self.graph.topological_sort()
        self.wl('char* cuda_buffer;')
        for node in node_list:
            if not isinstance(node, ops.Input):
                for i in range(len(node.output_types)):
                    self.wl(f'{cpp_type(node.output_types[i].dtype)}* {self.tensor_name(IndexNode(node, i))};')
        for tensor_name, tensor in self.node_constant_tensors.items():
            self.wl(f'{cpp_type(tensor.dtype)}* {tensor_name};')
        for node in node_list:
            global_params = node.get_global_params(self.node_name(node))
            for ty, name in global_params:
                self.wl(f'{ty} {name};')
        self.wl('')
        buffer_size = 0
        align_size = 256
        for node in node_list:
            if not isinstance(node, ops.Input):
                for i in range(len(node.output_types)):
                    if len(node.output_nodes[i]) == 1 and isinstance(node.output_nodes[i][0].node, ops.Output):
                        continue
                    buffer_size += align(node.output_types[i].size() * node.output_types[i].dtype.itemsize, align_size)
        for tensor in self.node_constant_tensors.values():
            buffer_size += align(tensor.size * tensor.dtype.itemsize, align_size)
        buffer_ptr = 0
        self.wl('void init()')
        self.block_start()
        self.wl(f'checkCudaErrors(cudaMalloc(&cuda_buffer, {buffer_size}));')
        for node in node_list:
            if not isinstance(node, ops.Input):
                for i in range(len(node.output_types)):
                    if len(node.output_nodes[i]) == 1 and isinstance(node.output_nodes[i][0].node, ops.Output):
                        continue
                    self.wl(f'{self.tensor_name(IndexNode(node, i))} = ({cpp_type(node.output_types[i].dtype)}*)(cuda_buffer + {buffer_ptr}); /* {node.output_types[i].shape} * {node.output_types[i].dtype.itemsize} */')
                    buffer_ptr += align(node.output_types[i].size() * node.output_types[i].dtype.itemsize, align_size)
        for tensor_name, tensor in self.node_constant_tensors.items():
            self.wl(f'{tensor_name} = ({cpp_type(tensor.dtype)}*)(cuda_buffer + {buffer_ptr}); /* {tensor.shape} * {tensor.dtype.itemsize} */')
            buffer_ptr += align(tensor.size * tensor.dtype.itemsize, align_size)
        for node in node_list:
            if isinstance(node, ops.Constant):
                self.wl(f'load_tensor("{os.path.join(self.codegen_dir, "constants", self.node_name(node))}.bin", {self.tensor_name(IndexNode(node, 0))}, {node.output_types[0].size() * node.output_types[0].dtype.itemsize});')
        for tensor_name, tensor in self.node_constant_tensors.items():
            self.wl(f'load_tensor("{os.path.join(self.codegen_dir, "constants", tensor_name)}.bin", {tensor_name}, {tensor.size * tensor.dtype.itemsize});')
        assert buffer_ptr == buffer_size
        for node in node_list:
            init_code = node.get_init_code(self.node_name(node))
            if init_code is not None:
                self.write(init_code)
        self.block_end()


        self.wl('')


    def write_kernels(self):
        node_list = self.graph.topological_sort()
        for node in node_list:
            if isinstance(node, (ops.Input, ops.Constant, ops.Output)):
                continue
            params = [f'{cpp_type(t.dtype)}* input{i}' for i, t in enumerate(node.input_types)]
            params += [f'{cpp_type(t.dtype)}* output{i}' for i, t in enumerate(node.output_types)]
            func_sig =  f"void kernel_{self.node_name(node)}({', '.join(params)})"
            self.wl(f'// {node}')
            self.wl(f'// {node.input_types}')
            self.wl(f'// {node.output_types}')
            self.write(node.get_cuda_code(func_sig, self.node_name(node)))
            self.wl('')
        
        run_params = []
        for node in self.graph.inputs:
            assert isinstance(node, ops.Input)
            run_params.append(f'{cpp_type(node.output_types[0].dtype)}* {self.tensor_name(IndexNode(node, 0))}')
        for i, node in enumerate(self.graph.outputs):
            assert isinstance(node, ops.Output)
            run_params.append(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}')
        
        self.wl(f'void run({", ".join(run_params)})')
        self.block_start()
        for node in node_list:
            if isinstance(node, (ops.Input, ops.Constant, ops.Output)):
                continue
            params = [self.tensor_name(input_node) for input_node in node.input_nodes]
            params += [self.tensor_name(IndexNode(node, i)) for i in range(len(node.output_types))]
            self.wl(f'kernel_{self.node_name(node)}({", ".join(params)});')
            # self.wl(f'checkCudaErrors(cudaDeviceSynchronize());') # for testing only

        self.block_end()
        self.wl('')


    def write_test_code(self):
        self.wl('int main()')
        self.block_start()
        self.wl('init();')
        for i, node in enumerate(self.graph.inputs):
            assert isinstance(node, ops.Input)
            size = node.output_types[0].size() * node.output_types[0].dtype.itemsize
            self.wl(f'{cpp_type(node.output_types[0].dtype)}* {self.tensor_name(IndexNode(node, 0))}; // input{i}')
            self.wl(f'checkCudaErrors(cudaMalloc(&{self.tensor_name(IndexNode(node, 0))}, {size}));')
            self.wl(f'load_tensor("{os.path.join(self.data_dir, f"input{i}")}.bin", {self.tensor_name(IndexNode(node, 0))}, {size});')
        for i, node in enumerate(self.graph.outputs):
            assert isinstance(node, ops.Output)
            size = node.input_types[0].size() * node.input_types[0].dtype.itemsize
            self.wl(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}; // output{i}')
            self.wl(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}_ref;')
            self.wl(f'{cpp_type(node.input_types[0].dtype)}* {self.tensor_name(node.input_nodes[0])}_cpu;')
            self.wl(f'checkCudaErrors(cudaMalloc(&{self.tensor_name(node.input_nodes[0])}, {size}));')
            self.wl(f'{self.tensor_name(node.input_nodes[0])}_ref = ({cpp_type(node.input_types[0].dtype)}*) malloc({size});')
            self.wl(f'{self.tensor_name(node.input_nodes[0])}_cpu = ({cpp_type(node.input_types[0].dtype)}*) malloc({size});')
            self.wl(f'load_tensor("{os.path.join(self.data_dir, f"output{i}")}.bin", {self.tensor_name(node.input_nodes[0])}_ref, {size}, false);')
        run_cmd = 'run(' + ', '.join([self.tensor_name(IndexNode(node, 0)) for node in self.graph.inputs] + [self.tensor_name(node.input_nodes[0]) for node in self.graph.outputs]) + ');'
        self.wl(run_cmd)
        for i, node in enumerate(self.graph.outputs):
            size = node.input_types[0].size() * node.input_types[0].dtype.itemsize
            self.wl(f'checkCudaErrors(cudaMemcpy({self.tensor_name(node.input_nodes[0])}_cpu, {self.tensor_name(node.input_nodes[0])}, {size}, cudaMemcpyDeviceToHost));')
        self.wl(f'checkCudaErrors(cudaDeviceSynchronize());')
        for i, node in enumerate(self.graph.outputs):
            self.wl(f'check_equal_cpu("{self.tensor_name(node.input_nodes[0])}", {self.tensor_name(node.input_nodes[0])}_cpu, {self.tensor_name(node.input_nodes[0])}_ref, {node.input_types[0].size()});')
            self.wl(f'print_tensor_cpu("{self.tensor_name(node.input_nodes[0])}_out", {self.tensor_name(node.input_nodes[0])}_cpu, {node.input_types[0].size()});')
            self.wl(f'print_tensor_cpu("{self.tensor_name(node.input_nodes[0])}_ref", {self.tensor_name(node.input_nodes[0])}_ref, {node.input_types[0].size()});')
        self.wl('int n_warmup = 100, n_run = 100;')
        self.wl('checkCudaErrors(cudaDeviceSynchronize());')
        self.wl('for (int i = 0; i < n_warmup; i++)')
        self.block_start()
        self.wl('auto start_time = std::chrono::system_clock::now();')
        self.wl(run_cmd)
        self.wl('checkCudaErrors(cudaDeviceSynchronize());')
        self.wl('auto end_time = std::chrono::system_clock::now();')
        self.wl('std::chrono::duration<double> time = end_time - start_time;')
        self.wl('printf("Warmup %d: %f ms\\n", i, time.count() * 1000);')
        self.block_end()
        self.wl('checkCudaErrors(cudaDeviceSynchronize());')
        self.wl('checkCudaErrors(cudaProfilerStart());')
        self.wl('auto start_time = std::chrono::system_clock::now();')
        self.wl('for (int i = 0; i < n_run; i++)')
        self.block_start()
        self.wl(run_cmd)
        self.block_end()
        self.wl('checkCudaErrors(cudaDeviceSynchronize());')
        self.wl('auto end_time = std::chrono::system_clock::now();')
        self.wl('checkCudaErrors(cudaProfilerStop());')
        self.wl('std::chrono::duration<double> time = end_time - start_time;')
        self.wl('printf("Time: %f ms\\n", time.count() / n_run * 1000);')
        self.wl('return 0;')
        self.block_end()

    def write_code(self):
        self.gen_constants()
        self.write_helper_code()
        self.write_init_code()
        self.write_kernels()
        self.write_test_code()


def codegen(graph, codegen_dir, data_dir):
    writer = CodeGen(graph, codegen_dir, data_dir)
    writer.write_code()
    print(writer.code_str)
    os.makedirs(codegen_dir, exist_ok=True)
    with open(os.path.join(codegen_dir, 'run.cu'), 'w') as f:
        f.write(writer.code_str)
    ret = os.system(f"nvcc -std=c++11 -arch=sm_70 -O3 -lcublas -lcudart -lcublasLt -g -o {os.path.join(codegen_dir, 'run')} {os.path.join(codegen_dir, 'run.cu')}")
    assert ret == 0, f"nvcc failed with code {ret}"

    # TODO: save constants
